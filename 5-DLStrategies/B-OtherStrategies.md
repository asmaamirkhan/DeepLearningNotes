# ğŸ“š Other Strategies of Deep Learning

## â° Multi-Task Learning
In short: We start simultaneously trying to have one NN do several things at same time and then each of these tasks helps all of the other tasks ğŸš€

**In other words:** Let's say that we want to build a detector to detect 4 classes of objects, instead of building 4 NN for each class, we can build one NN to detect the four classes ğŸ¤” (The output layer has 4 units) 

### ğŸ¤” When Is It Practical?
* ğŸ¤³ Training on a set of tasks that could benefit from having shared **lower level** features 
* â›± Amount of data we have for each task is quite similar (_sometimes_) â›±
* ğŸ¤— Can train a big enough NN to do well on all the tasks (instead of building a separate network fÄ±r each task) 

> ğŸ‘“ Multi task learning is used much less than transfer learning 

### ğŸ‘€ Visualization

<img src="../res/SingleTaskVsMultiTask.png" width="300"  />

## ğŸ´ End to End Deep Learning
- Briefly, there have been some data processing systems or learning systems that requires multiple stages of processing, 
- End to end learning can take all these multiple stages and replace it with just a single NN

> ğŸ‘©â€ğŸ”§ Long Story Short: breaking the big task into sub smaller tasks with the same NN

### â• Pros:

* ğŸ¦¸â€â™€ï¸ Shows the power of the data
* âœ¨ Less hand designing of components needed

### â– Cons:

* ğŸ¤¸â€â™€ï¸ May need large amount of data
* ğŸ” Excludes potentially useful hand designed components

### ğŸš© Guideline to Make Decision to Use It
**Key question:** do you have sufficient data to learn a function of the complexity needed to map x to y?

## ğŸ”ƒ End to End Learning vs Transfer Learning

<img src="../res/E2EVsTL.png" width="400"  />
