# ğŸš™ Transfer Learning
In short: Learning form one task and applying knowledge to seperate tasks ğŸ›°ğŸš™

## â“ What is Transfer Learning?
- ğŸ•µï¸â€â™€ï¸ Transfer learning is a machine learning technique where a model trained on one task is re-purposed on a second related task. 
- ğŸŒŸ In addition, it is an optimization method that allows rapid progress or improved performance when modeling the second task. 
- ğŸ¤¸â€â™€ï¸ Transfer learning only works in deep learning if the model features learned from the first task are general.

> Long story short: Rather than training a neural network form scratch we can instead download an open-source model that someone else has already trained on a huge dataset maybe for weeks and use these parameters as a starting point to train our model just a little bit more with the smaller dataset that we have âœ¨

## ğŸ’« Traditional ML vs Transfer Learning

<img src="../res/MLvsTL.png" width="450"  />


## ğŸ™„ Problem
Layers in a neural network can sometimes end up having similar weights and possible impact each other leading to **over-fitting**. With a big complex model it's a risk. So if you can imagine the dense layers can look a little bit like this.

We can drop out some neurons that has similar weights with neighbors, so that overfitting is being removed.

### ğŸ”ƒ Comparison
<p float="left">
    <img src="../res/NNWithoutDropout.JPG" width="300"  />
    <img src="../res/NNWithDropout.JPG" width="300"  />
</p>

> ğŸ¤¸â€â™€ï¸ An NN before and after dropout

<p float="left">
    <img src="../res/AccuracyWithoutDropOut.JPG" width="300"  />
    <img src="../res/AccuracyWithDropOut.JPG" width="300"  />
</p>

> âœ¨ Accuracy before and after dropout

## ğŸ¤” When is it practical?
It is practical when we have a lot of data for problem that we are transferring from and usually relativly less data for the problem we are transferring to ğŸ•µï¸â€

**More accurately:**

For `task A` to `task B`, it is sesnible to do transfer learning **from A to B** when:

* ğŸš© Task A and task B have the same output x 
* â­ We have a lot more data for `task A` than `task B` 
* ğŸ” Low level features from `task A` could be helpful for learning `task B` 


## ğŸ§ References
* [More about transfer learning in Tensorflow](https://www.tensorflow.org/tutorials/images/transfer_learning)
* [Understanding Dropout](https://www.youtube.com/watch?v=ARq74QuavAo)