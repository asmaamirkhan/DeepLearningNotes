# ðŸ”¥ Advanced Recurrent Neural Networks

## ðŸ”„ Bidirectional RNNs (BRNN)
- In many applications we want to output a prediction of y (t) which may depend on the whole input sequence
- Bidirectional RNNs combine an RNN that moves **forward** through time beginning from the start of the sequence with another RNN that moves **backward** through time beginning from the end of the sequence âœ¨

### ðŸ’¬ In Other Words
- Bidirectional recurrent neural networks(RNN) are really just putting two independent RNNs together. 
- The input sequence is fed in normal time order for one network, and in reverse time order for another. 
- The outputs of the two networks are usually concatenated at each time step.
- ðŸŽ‰ This structure allows the networks to have both backward and forward information about the sequence at every time step. 

### ðŸ‘Ž Disadvantages
We need the entire sequence of data efore you can make prediction anywhere.

>e.g: not suitable for real time speach recognition 

### ðŸ‘€ Visualization

<img src="../res/BRNN.png" width="600"  />


## ðŸ•¸ Deep RNNs
The computation in most RNNs can be decomposed into three blocks of parameters and associated transformations:
1. From the input to the hidden state, x(t) âž¡ a(t)
2. From the previous hidden state to the next hidden state, a(t-1) âž¡ a(t)
3. From the hidden state to the output, a(t) âž¡ y(t)

We can use multiple layers for each of the above transformations, which results in deep recurrent networks ðŸ˜‹

### ðŸ‘€ Visualization

<img src="../res/DeepRNN.png" width="600"  />