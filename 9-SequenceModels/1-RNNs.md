# ğŸ”„ Recurrent Neural Networks

## ğŸ” Definition
A class of neural networks that allow previous outputs to be used as inputs to the next layers
> They remember things they learnt during training âœ¨

## ğŸ§± Architecture

### ğŸ”¶ The Whole RNN Architecture
<img src="../res/RNNStructure2.png" width="600"  />

<img src="../res/RNNStructure.png" width="600"  />

### ğŸ§© An RNN Cell

<img src="../res/RNNCell.png" width="600"  />

Basic RNN cell. Takes as input xâŸ¨tâŸ© (current input) and a<sup>âŸ¨tâˆ’1âŸ©</sup> (previous hidden state containing information from the past), and outputs a<sup>âŸ¨tâŸ©</sup> which is given to the next RNN cell and also used to predict y<sup>âŸ¨tâŸ©</sup>

## â© Forward Propagation
**To find a<sup><<i>t</i>></sup>:**

<img src="../res/RNNForwardA.png" height="25"  />

**To find Å·<sup><<i>t</i>></sup>:**

<img src="../res/RNNForwardY.png" height="25"  />

### ğŸ‘€ Visualization

<img src="../res/RNNForwardVis.png" width="600"  />

## âª Back Propagation
**Loss Function is defined like the following**

<img src="../res/RNNOneLoss.png" height="25"  />

</br>

<img src="../res/RNNLoss.png" height="60"  />

## ğŸ¨ Types of RNNs
- **One-to-One** (Traditional ANN)
- **One-to-Many** (Music Generation)
- **Many-to-One** (Semantic Analysis)
- **Many-to-Many** T<sub>x</sub> = T<sub>y</sub> (Speech Recognition)
- **Many-to-Many** T<sub>x</sub> != T<sub>y</sub> (Machine Translation)

<img src="../res/RNNTypes.png" width="600"  />

# ğŸ”¥ Advanced Recurrent Neural Networks

## ğŸ”„ Bidirectional RNNs (BRNN)
- In many applications we want to output a prediction of y (t) which may depend on the whole input sequence
- Bidirectional RNNs combine an RNN that moves **forward** through time beginning from the start of the sequence with another RNN that moves **backward** through time beginning from the end of the sequence âœ¨

### ğŸ’¬ In Other Words
- Bidirectional recurrent neural networks(RNN) are really just putting two independent RNNs together. 
- The input sequence is fed in normal time order for one network, and in reverse time order for another. 
- The outputs of the two networks are usually concatenated at each time step.
- ğŸ‰ This structure allows the networks to have both backward and forward information about the sequence at every time step. 

### ğŸ‘ Disadvantages
We need the entire sequence of data efore you can make prediction anywhere.

>e.g: not suitable for real time speach recognition 

### ğŸ‘€ Visualization

<img src="../res/BRNN.png" width="600"  />


## ğŸ•¸ Deep RNNs
The computation in most RNNs can be decomposed into three blocks of parameters and associated transformations:
1. From the input to the hidden state, x(t) â¡ a(t)
2. From the previous hidden state to the next hidden state, a(t-1) â¡ a(t)
3. From the hidden state to the output, a(t) â¡ y(t)

We can use multiple layers for each of the above transformations, which results in deep recurrent networks ğŸ˜‹

### ğŸ‘€ Visualization

<img src="../res/DeepRNN.PNG" width="600"  />

## âŒ Problem: Vanishing Gradients with RNNs
- An RNN that process a sequence data with the size of 10,000 time steps, has 10,000 deep layers which is very hard to optimize ğŸ™„
- Same in Deep Neural Networks, deeper networks are getting into the vanishing gradient problem. 
- That also happens with RNNs with a long sequence size ğŸ›

### ğŸ§™â€â™€ï¸ Solutions
- Read [Part-2](./2-VanishingGradients.md) for my notes on Vanishing Gradients with RNNs ğŸ¤¸â€â™€ï¸

## ğŸ§ Read More
- [Recurrent Neural Networks Cheatsheet âœ¨](https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks#)
- [All About RNNs ğŸš€](https://medium.com/@jianqiangma/all-about-recurrent-neural-networks-9e5ae2936f6e)