---
description: â›“ â€Basics of Sequence Models
---

# ğŸŒ± Introduction

## â›“ Sequence Models In General
- Sequences are data structures where each example could be seen as a **series** of data points, for example ğŸ§:

|  Task                         | Input **X**        | Output **Y**          | Type                   |
| ----------------------------- | ------------------ | --------------------- | ---------------------- |
| ğŸ’¬ Speech Recognition         | Wave sequence      | Text sequence         | Sequence-to-Sequence   |
| ğŸ¶ Music Generation           |  Nothing / Integer | Wave Sequence         | One-to_Sequence        |
| ğŸ’Œ Sentiment Classification   | Text Sequence      | Integer Rating (1â¡5) | Sequence-to-One        |
| ğŸ”  Machine Translation        | Text Sequence      | Text Sequence         | Sequence-to-Sequence   |
| ğŸ“¹ Video Activity Recognition | Video Frames       | Label                 | Sequence-to-One        |

> - Since we have labeled data **X** and **Y** so all of these tasks are addressed as _Supervised Learning_ ğŸ‘©â€ğŸ«
> - Even in Sequence-to-Sequence tasks lengths of input and output can be different â—

## ğŸ¤” Why Do We Need Sequence Models?
- Machine learning algorithms typically require the text input to be represented as a fixed-length vector ğŸ™„
- Thus, to model sequences, we need a specific learning framework able to:
  - âœ” Deal with variable-length sequences
  - âœ” Maintain sequence order
  - âœ” Keep track of long-term dependencies rather than cutting input data too short
  - âœ” Share parameters across the sequence (so not **re-learn** things across the sequence)

## ğŸ‘©â€ğŸ’» My Codes
- [ğŸ’¬ Text Classification](A-TextClassification.ipynb)