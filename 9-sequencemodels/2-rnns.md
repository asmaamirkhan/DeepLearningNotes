# ğŸ”„ Recurrent Neural Networks

## ğŸ”„ Recurrent Neural Networks

### ğŸ” Definition

A class of neural networks that allow previous outputs to be used as inputs to the next layers

> They remember things they learned during training âœ¨

### ğŸ§± Architecture

#### ğŸ”¶ The Whole RNN Architecture

![](../.gitbook/assets/rnnstructure2.png)

![](../.gitbook/assets/rnnstructure.png)

#### ğŸ§© An RNN Cell

![](../.gitbook/assets/rnncell.png)

Basic RNN cell. Takes as input $$x^{âŸ¨tâŸ©}$$ \(current input\) and $$a^{âŸ¨tâˆ’1âŸ©}$$ \(previous hidden state containing information from the past\), and outputs $$a^{âŸ¨tâŸ©}$$ which is given to the next RNN cell and also used to predict $$y^{âŸ¨tâŸ©}$$

### â© Forward Propagation

**To find** $$a^{<t>}$$**:**

$$a^{<t>}=g(W_{aa}a^{<t-1>}+W_{ax}x^{<t>}+b_a)$$

**To find** $$\hat{y}^{<t>}$$**:**

$$\hat{y}^{<t>} = g(W_{ya}a^{<t>}+b_y)$$

#### ğŸ‘€ Visualization

![](../.gitbook/assets/rnnforwardvis.png)

### âª Back Propagation

**Loss Function is defined like the following**

$$L^{<t>}(\hat{y}^{<t>}, y^{<t>})=-y^{<t>}log(\hat{y})-(1-y^{<t>})log(1-\hat{y}^{<t>})$$

$$L(\hat{y},y)=\sum_{t=1}^{T_y}L^{<t>}(\hat{y}^{<t>}, y^{<t>})$$

### ğŸ¨ Types of RNNs

* 1ï¸âƒ£ â¡ 1ï¸âƒ£ **One-to-One** \(Traditional ANN\)
* 1ï¸âƒ£ â¡ ğŸ”¢ **One-to-Many** \(Music Generation\)
* ğŸ”¢ â¡ 1ï¸âƒ£ **Many-to-One** \(Semantic Analysis\)
* ğŸ”¢ â¡ ğŸ”¢ **Many-to-Many** $$T_x = T_y$$ \(Speech Recognition\)
* ğŸ”¢ â¡ ğŸ”¢ **Many-to-Many** $$T_x \neq T_y$$ \(Machine Translation\)

![](../.gitbook/assets/rnntypes.png)

## ğŸ”¥ Advanced Recurrent Neural Networks

### ğŸ”„ Bidirectional RNNs \(BRNN\)

* In many applications we want to output a prediction of $$y^{(t)}$$ which may depend on the whole input sequence
* Bidirectional RNNs combine an RNN that moves **forward** through time beginning from the start of the sequence with another RNN that moves **backward** through time beginning from the end of the sequence âœ¨

#### ğŸ’¬ In Other Words

* Bidirectional recurrent neural networks\(RNN\) are really just putting two independent RNNs together. 
* The input sequence is fed in normal time order for one network, and in reverse time order for another. 
* The outputs of the two networks are usually concatenated at each time step.
* ğŸ‰ This structure allows the networks to have both backward and forward information about the sequence at every time step. 

#### ğŸ‘ Disadvantages

We need the entire sequence of data before we can make prediction anywhere.

> e.g: not suitable for real time speech recognition

#### ğŸ‘€ Visualization

![](../.gitbook/assets/brnn.png)

### ğŸ•¸ Deep RNNs

The computation in most RNNs can be decomposed into three blocks of parameters and associated transformations: 1. From the input to the hidden state, $$x^{(t)}$$ â¡ $$a^{(t)}$$ 2. From the previous hidden state to the next hidden state, $$a^{(t-1)}$$ â¡ $$a^{(t)}$$ 3. From the hidden state to the output, $$a^{(t)}$$ â¡ $$y^{(t)}$$

We can use multiple layers for each of the above transformations, which results in deep recurrent networks ğŸ˜‹

#### ğŸ‘€ Visualization

![](../.gitbook/assets/deeprnn.PNG)

### âŒ Problem: Vanishing Gradients with RNNs

* An RNN that processes a sequence data with the size of 10,000 time steps, has 10,000 deep layers which is very hard to optimize ğŸ™„
* Same in Deep Neural Networks, deeper networks are getting into the vanishing gradient problem ğŸ¥½
* That also happens with RNNs with a long sequence size ğŸ›

#### ğŸ§™â€â™€ï¸ Solutions

* Read [Part-2](https://github.com/asmaamirkhan/DeepLearningNotes/tree/d20deec6ead832c6650a0d761212dfafef64b302/9-SequenceModels/2-VanishingGradients.md) for my notes on Vanishing Gradients with RNNs ğŸ¤¸â€â™€ï¸

### ğŸ§ Read More

* [Recurrent Neural Networks Cheatsheet âœ¨](https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks#)
* [All About RNNs ğŸš€](https://medium.com/@jianqiangma/all-about-recurrent-neural-networks-9e5ae2936f6e)

