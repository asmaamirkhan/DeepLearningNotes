---
description: Training Custom Object Detector Step by Step
---

# ğŸ¤– TensorFlow Object Detection API

## ğŸŒ± Introduction

* âœ¨ _Tensorflow_ object detection API is a powerful tool that allows us to create custom object detectors depending on pre-trained, fine tuned models even if we don't have strong AI background or strong _TensorFlow_ knowledge.
* ğŸ’â€â™€ï¸ Building models depending on pre-trained models saves us a lot of time and labor since we are using models that maybe trained for weeks using very strong machines, this principle is called [**Transfer Learning**](../popular-strategies-of-deep-learning/transfer-learning.md)**.**
* ğŸ—ƒï¸ As a data set I will show you how to use _OpenImages_ data set and converting its data to _TensorFlow_-friendly format.
* ğŸ€ You can find this article on [**Medium**](https://medium.com/@asmaamirkhan.am/training-custom-object-detector-step-by-step-5f6f4d75b494) too.

## ğŸš© Development Pipeline

1. [ğŸ‘©â€ğŸ’» Environment Preparation](tensorflow-object-detection-api.md#environment-preparation)
2. [ğŸ–¼ï¸ Image acquiring](tensorflow-object-detection-api.md#image-acquiring)
3. [ğŸ¤¹â€â™€ï¸ Image Organization](tensorflow-object-detection-api.md#image-organization)
4. [ğŸ¤– Model Selecting](tensorflow-object-detection-api.md#model-selecting)
5. [ğŸ‘©â€ğŸ”§ Model Configuration](tensorflow-object-detection-api.md#model-configuration)
6. [ğŸ‘¶ Training](tensorflow-object-detection-api.md#training)
7. [ğŸ‘®â€â™€ï¸ Evaluation](tensorflow-object-detection-api.md#evaluation)
8. [ğŸ‘’ Model Exporting](tensorflow-object-detection-api.md#model-exporting)
9. [ğŸ“± Converting to tflite](tensorflow-object-detection-api.md#converting-to-tflite)

{% hint style="info" %}
ğŸ¤• While you are applying the instructions if you get errors you can check out [ğŸ Common Issues](tensorflow-object-detection-api.md#common-issues) section at the end of the article
{% endhint %}

## ğŸ‘©â€ğŸ’» Environment Preparation

### ğŸ”¸ Environment Info

| ğŸ’» Platform | ğŸ·ï¸ Version |
| :--- | :--- |
| Python version | 3.7 |
| TensorFlow version | 1.15 |

### ğŸ¥¦ Conda env Setting

#### ğŸ”® Create new env

* ğŸ¥¦ Install [**Anaconda**](https://www.anaconda.com/)
* ğŸ’» Open cmd and run:

```bash
# conda create -n <ENV_NAME> python=<REQUIRED_VERSION>
conda create -n tf1 python=3.7
```

#### â–¶ï¸ Activate the new env

```bash
# conda activate <ENV_NAME>
conda activate tf1
```

### ğŸ”½ Install Packages

#### ğŸ’¥ GPU vs CPU Computing

| ğŸš™ CPU | ğŸš€ GPU |
| :--- | :--- |
| **Brain** of computer | **Brawn** of computer |
| Very few complex cores | hundreds of simpler cores with parallel architecture |
| single-thread performance optimization | thousands of concurrent hardware threads |
| Can do a bit of everything, but not great at much | Good for math heavy processes |

#### ğŸš€  Installing TensorFlow

{% tabs %}
{% tab title="ğŸš€ GPU" %}
```bash
conda install tensorflow-gpu=1.15
```
{% endtab %}

{% tab title="ğŸš™ CPU" %}
```bash
conda install tensorflow=1.15
```
{% endtab %}
{% endtabs %}

#### ğŸ“¦ Installing other packages

```bash
conda install pillow Cython lxml jupyter matplotlib
```

```bash
conda install -c anaconda protobuf
```

### ğŸ¤– Downloading models repository

#### ğŸ¤¸â€â™€ï¸ Cloning from GitHub

* A repository that contains required utils for training and evaluation process
* Open CMD and run in `E` disk and run:

```bash
# note that every time you open CMD you have 
# to activate your env again by running: 
# E:\>conda activate tf1
(tf1) E:\>git clone https://github.com/tensorflow/models.git
(tf1) E:\>cd models/research
```

{% hint style="warning" %}
ğŸ§ I assume that you are running your commands under `E` disk,
{% endhint %}

#### ğŸ”ƒ Compiling Protobufs

{% tabs %}
{% tab title="ğŸ’» Windows" %}
```bash
# under (tf1) E:\models\research>
for /f %i in ('dir /b object_detection\protos\*.proto') do protoc object_detection\protos\%i --python_out=.
```
{% endtab %}

{% tab title="ğŸ§ Linux" %}
```bash
# under /models/research
$ protoc object_detection/protos/*.proto --python_out=.
```
{% endtab %}
{% endtabs %}

#### ğŸ“¦ Compiling Packages

```bash
# under (tf1) E:\models\research>
python setup.py build
python setup.py install
```

#### ğŸš© Setting Python Path Temporarily

{% tabs %}
{% tab title="ğŸ’» Windows" %}
```bash
# under (tf1) E:\models\research> or anywhere ğŸ˜…
set PYTHONPATH=E:\models\research;E:\models\research\slim
```
{% endtab %}

{% tab title="ğŸ§ Linux" %}
```bash
# under /models/research
$ export PYTHONPATH=`pwd`:`pwd`/slim
```
{% endtab %}
{% endtabs %}

{% hint style="info" %}
ğŸ‘®â€â™€ï¸ Every time you open CMD you have to set `PYTHONPATH` again
{% endhint %}

### ğŸ‘©â€ğŸ”¬ Installation Test

ğŸ§ Check out that every thing is done

#### ğŸ’» Command

```bash
# under (tf1) E:\models\research>
python object_detection/builders/model_builder_tf1_test.py
```

#### ğŸ‰ Expected Output

```bash
Ran 17 tests in 0.833s

OK (skipped=1)
```

## ğŸ–¼ï¸ Image Acquiring

### ğŸ‘®â€â™€ï¸ Directory Structure

* ğŸ—ï¸ I suppose that you created a structure like:

```graphql
E:
|___ models
|___ demo
      |___ annotations
      |___ eval
      |___ images
      |___ inference
      |___ OIDv4_ToolKit
      |___ OpenImagesTool
      |___ pre_trainded_model
      |___ scripts
      |___ training
```

| ğŸ“‚ Folder | ğŸ“ƒ Description |
| :--- | :--- |
| ğŸ¤– `models` | the repo [**here**](https://github.com/tensorflow/models) |
| ğŸ“„ `annotations` | will contain generated `.csv` and `.record` files |
| ğŸ‘®â€â™€ï¸ `eval` | will contain results of evaluation |
| ğŸ–¼ï¸ `images` | will contain image data set |
| â–¶ï¸ `inference` | will contain exported models after training |
| ğŸ”½ `OIDv4_ToolKit` | the repo [**here**](https://github.com/EscVM/OIDv4_ToolKit) \(_OpenImages_ Downloader\) |
| ğŸ‘©â€ğŸ”§ `OpenImagesTool` | the repo [**here**](https://github.com/asmaamirkhan/OpenImagesTool) \(_OpenImages_ Organizer\) |
| ğŸ‘©â€ğŸ«`pre_trained_model` | will contain files of _TensorFlow_ model that we will retrain |
| ğŸ‘©â€ğŸ’» `scripts` | will contain scripts that we will use for pre-processing and training processes |
| ğŸš´â€â™€ï¸ `training` | will contain generated check points during training |

### ğŸš€ OpenImages Dataset

* ğŸ•µï¸â€â™€ï¸ You can get images in various methods 
* ğŸ‘©â€ğŸ« I will show process of organizing OpenImages data set
* ğŸ—ƒï¸ OpenImages is a huge data set contains annotated images of 600 objects
* ğŸ” You can explore images by categories from [**here**](https://storage.googleapis.com/openimages/web/visualizer/index.html?set=train&type=segmentation&r=false&c=%2Fm%2F0420v5) 

{% embed url="https://storage.googleapis.com/openimages/web/index.html" caption="" %}

### ğŸ¨ Downloading By Category

[**OIDv4\_Toolkit**](https://github.com/EscVM/OIDv4_ToolKit) is a tool that we can use to download OpenImages dataset by category and by set \(test, train, validation\)

ğŸ’» To clone and build the project, open CMD and run:

```bash
# under (tf1) E:\demo>
git clone https://github.com/EscVM/OIDv4_ToolKit.git
cd OIDv4_ToolKit

# under (tf1) E:\demo\OIDv4_ToolKit>
pip install -r requirements.txt
```

â¬ To start downloading by category:

```bash
# python main.py downloader --classes <OBJECT_LIST> --type_csv <TYPE>
# TYPE: all | test | train | validation 
# under (tf1) E:\demo\OIDv4_ToolKit>
python main.py downloader --classes Apple Orange --type_csv validation
```

{% hint style="warning" %}
ğŸ‘®â€â™€ï¸ If object name consists of 2 parts then write it with `'_', e.g.` Bell\_pepper
{% endhint %}

## ğŸ¤¹â€â™€ï¸ Image Organization

### ğŸ”® OpenImagesTool

* ğŸ‘©â€ğŸ’» [**OpenImagesTool**](https://github.com/asmaamirkhan/OpenImagesTool) is a tool to convert OpenImages images and annotations to _TensorFlow_-friendly structure.
* ğŸ™„ _OpenImages_ provides annotations ad `.txt` files in a format like:`<OBJECT_NAME> <XMIN> <YMIN> <XMAX> <YMAX>` which is not compatible with _TensorFlow_ that requires _VOC_ annotation format
* ğŸ’« To do that synchronization we can do the following  

ğŸ’» To clone and build the project, open _CMD_ and run:

```bash
# under (tf1) E:\demo>
git clone https://github.com/asmaamirkhan/OpenImagesTool.git
cd OpenImagesTool/src
```

### ğŸ’» Applying Organizing

ğŸš€ Now, we will convert images and annotations that we have downloaded and save them to `images` folder

```bash
# under (tf1) E:\demo\OpenImagesTool\src> 
# python script.py -i <INPUT_PATH> -o <OUTPUT_PATH>
python script.py -i E:\pre_trainded_model\OIDv4_ToolKit\OID\Dataset -o E:\pre_trainded_model\images
```

{% hint style="info" %}
ğŸ‘©â€ğŸ”¬ _OpenImagesTool_ adds validation images to training set by default, if you wand to disable this behavior you can add `-v` flag to the command.
{% endhint %}

### ğŸ·ï¸ Creating Label Map

* â›“ï¸ `label_map.pbtxt` is a file that maps object names to corresponded IDs
* â• Create `label_map.pbtxt`file under annotations folder and open it in a text editor
* ğŸ–Šï¸ Write your objects names and IDs in the following format

```javascript
item {
    id: 1
    name: 'Hamster'
}

item {
    id: 2
    name: 'Apple'
}
```

{% hint style="info" %}
`ğŸ‘®â€â™€ï¸ id:0` is reserved for background, so don' t use it

ğŸ Related error: `ValueError: Label map id 0 is reserved for the background label`
{% endhint %}

### ğŸ­ Generating CSV Files

* ğŸ”„ Now we have to convert `.xml` files to csv file
* ğŸ”» Download the script [**xml\_to\_csv.py**](https://github.com/asmaamirkhan/DeepLearningNotes/blob/master/8-objectdetection/xml_to_csv.py)  script and save it under `scripts` folder
* ğŸ’» Open CMD and run:

#### ğŸ‘©â€ğŸ”¬ Generating train csv file

```bash
# under (tf1) E:\demo\scripts>
python xml_to_csv.py -i E:\demo\images\train -o E:\demo\annotations\train_labels.csv
```

#### ğŸ‘©â€ğŸ”¬ Generating test csv file

```bash
# under (tf1) E:\demo\scripts>
python xml_to_csv.py -i E:\demo\images\test -o E:\demo\annotations\test_labels.csv
```

### ğŸ‘©â€ğŸ­ Generating TF Records

* ğŸ™‡â€â™€ï¸ Now, we will generate tfrecords that will be used in training precess
* ğŸ”» Download [**generate\_tfrecords.py**](https://github.com/asmaamirkhan/DeepLearningNotes/blob/master/8-objectdetection/generate_tfrecords.py) script and save it under `scripts` folder

#### ğŸ‘©â€ğŸ”¬ Generating train tfrecord

```bash
# under (tf1) E:\demo\scripts>
# python generate_tfrecords.py --label_map=<PATH_TO_LABEL_MAP> 
# --csv_input=<PATH_TO_CSV_FILE> --img_path=<PATH_TO_IMAGE_FOLDER>
# --output_path=<PATH_TO_OUTPUT_FILE>
python generate_tfrecords.py --label_map=E:/demo/annotations/label_map.pbtxt --csv_input=E:\demo\annotations\train_labels.csv --img_path=E:\demo\images\train --output_path=E:\demo\annotations\train.record
```

#### ğŸ‘©â€ğŸ”¬ Generating test tfrecord

```bash
# under (tf1) E:\demo\scripts>
python generate_tfrecords.py --label_map=E:/demo/annotations/label_map.pbtxt --csv_input=E:\demo\annotations\test_labels.csv --img_path=E:\demo\images\test --output_path=E:\demo\annotations\test.record
```

## ğŸ¤– Model Selecting

* ğŸ‰ [**TensorFLow Object Detection Zoo**](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf1_detection_zoo.md#coco-trained-models) provides a lot of pre-trained models
* ğŸ•µï¸â€â™€ï¸ Models differentiate in terms of accuracy and speed, you can select the suitable model due to your priorities
* ğŸ’¾ Select a model, extract it and save it under `pre_trained_model` folder
* ğŸ‘€ Check out my notes [**here**](https://dl.asmaamir.com/8-objectdetection) to get insight about differences between popular models 

## ğŸ‘©â€ğŸ”§ Model Configuration

### â¬ Downloading config File

* ğŸ˜ We have downloaded the models \(pre-trained weights\) but now we have to download configuration file that contains training parameters and settings
* ğŸ‘®â€â™€ï¸ Every model in TensorFlow Object Detection Zoo has a configuration file presented [**here**](https://github.com/tensorflow/models/tree/master/research/object_detection/samples/configs)
* ğŸ’¾ Download the config file that corresponds to the models you have selected and save it under `training` folder

### ğŸ‘©â€ğŸ”¬ Updating config File

You have to update the following lines:

{% hint style="info" %}
ğŸ™„ Take a look at [Loss exploding issue]()
{% endhint %}

```javascript
// number of classes
num_classes: 1 // set it to total number of classes you have

// path of pre-trained checkpoint
fine_tune_checkpoint: "E:/demo/pre_trained_model/ssd_mobilenet_v1_quantized_300x300_coco14_sync_2018_07_18/model.ckpt"

// path to train tfrecord
tf_record_input_reader {
    input_path: "E:/demo/annotations/train.record"
}

// number of images that will be used in evaluation process
eval_config: {
  metrics_set: "coco_detection_metrics"
  use_moving_averages: false
  // I suggest setting it to total number of testing set to get accurate results
  num_examples: 11193
}

eval_input_reader: {
  tf_record_input_reader {
    // path to test tfrecord
    input_path: "E:/demo/annotations/test.record"
  }
  // path to label map
  label_map_path: "E:/demo/annotations/label_map.pbtxt"
  // set it to true if you want to shuffle test set at each evaluation   
  shuffle: false
  num_readers: 1
}
```

{% hint style="info" %}
ğŸ¤¹â€â™€ï¸ If you give the whole test set to evaluation process then shuffle functionality won't affect the results, it will only give you different examples on TensorBoard
{% endhint %}

## ğŸ‘¶ Training

* ğŸ‰ Now we have done all preparations
* ğŸš€ Let the computer start learning
* ğŸ’» Open CMD and run:

```bash
# under (tf1) E:\models\research\object_detection\legacy> 
# python train.py --train_dir=<DIRECTORY_TO_SAVE_CHECKPOINTS> 
# --pipeline_config_path=<PATH_TO_CONFIG_FILE>
python train.py --train_dir=E:/demo/training --pipeline_config_path=E:/demo/training/ssd_mobilenet_v1_quantized_300x300_coco14_sync.config
```

* ğŸ• This process will take long \(You can take a nap ğŸ¤­, but a long nap ğŸ™„\)
* ğŸ•µï¸â€â™€ï¸ While model is being trained you will see loss values on CMD
* âœ‹ You can stop the process when the loss value achieves a good value \(under 1\)  

## ğŸ‘®â€â™€ï¸ Evaluation

### ğŸ³ Evaluating Script

* ğŸ¤­ After training process is done, let's do an exam to know how good \(or bad ğŸ™„\) is our model doing
* ğŸ© The following command will use the model on whole test set and after that print the results, so that we can do error analysis.
* ğŸ’» So that, open CMD and run:

```bash
# under (tf1) E:\models\research\object_detection\legacy> 
# python eval.py --logtostderr --pipeline_config_path=<PATH_TO_CONFIG_FILE>
# --checkpoint_dir=<DIRECTORY_OF_CHECKPOINTS> --eval_dir=<DIRECTORY_TO_SAVE_EVAL_RESULTS>
python eval.py --pipeline_config_path=E:/demo/training/ssd_mobilenet_v1_quantized_300x300_coco14_sync.config --checkpoint_dir=E:/demo/training --eval_dir=E:/demo/eval
```

### ğŸ‘€ Visualizing Results

* âœ¨ To see results on charts and images we can use TensorBoard for better analyzing
* ğŸ’» Open CMD and run:

#### ğŸ‘©â€ğŸ« Training Values Visualization

* ğŸ§ Here you can see graphs of loss, learning rate and other values
* ğŸ¤“ And much more \(You can investigate tabs at the top\)
* ğŸ˜‹ It is feasable to use it while training \(and exciting ğŸ¤©\)

```bash
# under (tf1) E:\>
tensorboard --logdir=E:/demo/tarining
```

#### ğŸ‘®â€â™€ï¸ Evaluation Values Visualization

* ğŸ‘€ Here you can see images from your test set with corresponded predictions
* ğŸ¤“ And much more \(You can inspect tabs at the top\)
* â— You must use this after running evaluation script

```bash
# under (tf1) E:\>
tensorboard --logdir=E:/demo/eval
```

* ğŸ” See the visualized results on [localhost:6006](http://localhost:6006/) and 
* ğŸ§ You can inspect numerical values from report on terminal, result example:

```bash
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.708
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.984
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.868
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.289
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.623
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.767
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.779
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.781
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.781
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.300
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.703
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.824
```

* ğŸ¨ If you want to get metric report for each class you have to change evaluating protocol to _pascal metrics_ by configuring  `metrics_set` in `.config` file:

```javascript
eval_config: {
  ...
  metrics_set: "weighted_pascal_voc_detection_metrics"
  ...
}
```

## ğŸ‘’ Model Exporting

* ğŸ”§ After training and evaluation processes are done, we have to make the model in such a format that we can use
* ğŸ¦º For now, we have only checkpoints, so that we have to export `.pb` file
* ğŸ’» So, open  CMD and run:

```bash
# under (tf1) E:\models\research\object_detection>
# python export_inference_graph.py --input_type image_tensor 
# --pipeline_config_path <PATH_TO_CONFIG_FILE> 
# --trained_checkpoint_prefix <PATH_TO_LAST_CHECKPOINT>
# --output_directory <PATH_TO_SAVE_EXPORTED_MODEL>
python export_inference_graph.py --input_type image_tensor --pipeline_config_path=E:/demo/training/ssd_mobilenet_v1_quantized_300x300_coco14_sync.config --trained_checkpoint_prefix E:/demo/training/model.ckpt-16438 --output_directory E:/demo/inference/ssd_v1_quant
```

* If you are using SSD and planning to convert it to tflite later you have to run

```bash
# under (tf1) E:\models\research\object_detection>
# python export_tflite_ssd_graph.py --input_type image_tensor 
# --pipeline_config_path <PATH_TO_CONFIG_FILE> 
# --trained_checkpoint_prefix <PATH_TO_LAST_CHECKPOINT>
# --output_directory <PATH_TO_SAVE_EXPORTED_MODEL>
python export_tflite_ssd_graph.py --input_type image_tensor --pipeline_config_path=E:/demo/training/ssd_mobilenet_v1_quantized_300x300_coco14_sync.config --trained_checkpoint_prefix E:/demo/training/model.ckpt-16438 --output_directory E:/demo/inference/ssd_v1_quant
```

## ğŸ“± Converting to tflite

* ğŸ’â€â™€ï¸ If you want to use the model in mobile apps or tflite supported embedded devices you have to convert `.pb` file to `.tflite` file

### ğŸ“™ About TFLite

* ğŸ“± TensorFlow Lite is TensorFlowâ€™s lightweight solution for mobile and embedded devices. 
* ğŸ§ It enables on-device machine learning inference with low latency and a small binary size.
* ğŸ˜ TensorFlow Lite uses many techniques for this such as quantized kernels that allow smaller and faster \(fixed-point math\) models.
* ğŸ“ [**Official site**](https://www.tensorflow.org/lite)

**ğŸ« Converting Command**

* ğŸ’» To apply converting open CMD and run:

```bash
# under (tf1) E:\>
# toco --graph_def_file=<PATH_TO_PB_FILE>
# --output_file=<PATH_TO_SAVE> --input_shapes=<INPUT_SHAPES>
# --input_arrays=<INPUT_ARRAYS> --output_arrays=<OUTPUT_ARRAYS>
# --inference_type=<QUANTIZED_UINT8|FLOAT> --change_concat_input_ranges=<true|false>
# --alow_custom_ops 
# args for QUANTIZED_UINT8 inference
# --mean_values=<MEAN_VALUES> std_dev_values=<STD_DEV_VALUES> 
toco --graph_def_file=E:\demo\inference\ssd_v1_quant\tflite_graph.pb --output_file=E:\demo\tflite\ssd_mobilenet.tflite --input_shapes=1,300,300,3 --input_arrays=normalized_input_image_tensor --output_arrays=TFLite_Detection_PostProcess,TFLite_Detection_PostProcess:1,TFLite_Detection_PostProcess:2,TFLite_Detection_PostProcess:3 --inference_type=QUANTIZED_UINT8 --mean_values=128 --std_dev_values=128 --change_concat_input_ranges=false --allow_custom_ops
```

## ğŸ Common Issues

### ğŸ¥… nets module issue

`ModuleNotFoundError: No module named 'nets'`

This means that there is a problem in setting `PYTHONPATH`, try to run:

```bash
(tf1) E:\models\research>set PYTHONPATH=E:\models\research;E:\models\research\slim
```

### ğŸ—ƒï¸ tf\_slim module issue

`ModuleNotFoundError: No module named 'tf_slim'`

This means that tf\_slim module is not installed, try to run:

```bash
(tf1) E:\models\research>pip install tf_slim
```

### ğŸ—ƒï¸ Allocation error

```bash
2020-08-11 17:44:00.357710: I tensorflow/core/common_runtime/bfc_allocator.cc:929] Stats: 
Limit:                 10661327
InUse:                 10656704
MaxInUse:              10657688
NumAllocs:                 2959
MaxAllocSize:           3045064
```

For me it is fixed by minimizing batch\_size in `.config` file, it is related to your computations resources

```javascript
train_config: {
  ....
  batch_size: 128
  ....
}
```

### â— no such file or directory error

`train.py tensorflow.python.framework.errors_impl.notfounderror no such file or directory`

* ğŸ™„ For me it was a typo in train.py command
* [ğŸ“ Related discussion 1](https://github.com/tensorflow/models/issues/3762)
* [ğŸ“ Related discussion 2](https://github.com/tensorflow/models/issues/3954)

### ğŸ¤¯ LossTensor is inf issue

`LossTensor is inf or nan. : Tensor had NaN values`

* ğŸ‘€ Related discussion is [**here**](https://github.com/tensorflow/models/issues/1881), it is common that it is an annotation problem
* ğŸ™„ Maybe there is some bounding boxes outside the image boundaries
* ğŸ¤¯ The solution for me was minimizing batch size in `.config` file

### ğŸ™„ Ground truth issue

`The following classes have no ground truth examples`

* ğŸ‘€ Related discussion is [**here**](https://github.com/tensorflow/models/issues/1936)
* ğŸ‘©â€ğŸ”§ For me it was a misspelling issue in `label_map` file, 
* ğŸ™„ Pay attention to small and capital letters

### ğŸ·ï¸ labelmap issue

`ValueError: Label map id 0 is reserved for the background label`

* ğŸ‘®â€â™€ï¸ id:0 is reserved for background, We can not use it for objects
* ğŸ†” start IDs from 1

### ğŸ”¦ No Variable to Save issue

`Value Error: No Variable to Save`

* ğŸ‘€ Related solution is [**here**](https://ai.yemreak.com/tensorflow-object-detection-api/hata-notlari#value-error-no-variable-to-save)
* ğŸ‘©â€ğŸ”§ Adding the following line to `.config` file solved the problem

```javascript
train_config: {
  ...
  fine_tune_checkpoint_type:  "detection"
  ...
}
```

### ğŸ§ª pycocotools module issue

`ModuleNotFoundError: No module named 'pycocotools'`

{% tabs %}
{% tab title="ğŸ’» Windows" %}
* ğŸ‘€ Related discussion is [**here**](https://github.com/tensorflow/models/issues/3367)
* ğŸ‘©â€ğŸ”§ Applying the downloading instructions provided [**here**](https://github.com/philferriere/cocoapi) solved the problem for me \(on Windows 10\) 
{% endtab %}

{% tab title="ğŸ§ Linux" %}
```bash
$ conda install -c conda-forge pycocotools
```
{% endtab %}
{% endtabs %}

### ğŸ¥´ pycocotools type error issue

`pycocotools typeerror: object of type cannot be safely interpreted as an integer.`

* ğŸ‘©â€ğŸ”§ I solved the problem by editing the following lines in `cocoeval.py` script under _pycocotools_ package \(by adding casting\)
* ğŸ‘®â€â™€ï¸ Make sure that you are editting the package in you env not in other env.

```python
self.iouThrs = np.linspace(.5, 0.95, int(np.round((0.95 - .5) / .05)) + 1, endpoint=True)
self.recThrs = np.linspace(.0, 1.00, int(np.round((1.00 - .0) / .01)) + 1, endpoint=True)
```

### ğŸ’£ Loss Exploding

```text
INFO:tensorflow:global step 440: loss = 2106942657570782838784.0000 (0.405 sec/step)
INFO:tensorflow:global step 440: loss = 2106942657570782838784.0000 (0.405 sec/step)
INFO:tensorflow:global step 441: loss = 7774169971762292326400.0000 (0.401 sec/step)
INFO:tensorflow:global step 441: loss = 7774169971762292326400.0000 (0.401 sec/step)
INFO:tensorflow:global step 442: loss = 25262924095336287830016.0000 (0.404 sec/step)
INFO:tensorflow:global step 442: loss = 25262924095336287830016.0000 (0.404 sec/step)
```

ğŸ™„ For me there were 2 problems:

**First:**

* Some of annotations were wrong and overflow the image \(e.g. xmax &gt; width\)
* I could check that by inspecting `.csv` file
* Example:

| filename | width | height | class | xmin | ymin | xmax | ymax |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| 104.jpg | 640 | **480** | `class_1` | 284 | 406 | 320 | **492** |

**Second:**

* Learning rate in `.config` file is too big \(the default value was big ğŸ™„\)
* The following values are valid and tested on `mobilenet_ssd_v1_quantized` \(Not very good ğŸ™„\)

```javascript
learning_rate: {
  cosine_decay_learning_rate {
    learning_rate_base: .01
    total_steps: 50000
    warmup_learning_rate: 0.005
    warmup_steps: 2000
  }
}
```

* [ğŸ‘€ Related Discussion 1](https://github.com/tensorflow/models/issues/3868)
* [ğŸ‘€ Related Discussion 1](https://github.com/tensorflow/models/issues/8423)

### ğŸ¥´ Getting convolution Failure

```bash
Error : Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.
```

* It may be a _Cuda_ version incompatibility issue
* For me it was a memory issue and I solved it by adding the following line to `train.py` script

```python
os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'
```

* [ğŸ‘€ Related discussion](https://github.com/tensorflow/tensorflow/issues/24828)
* [ğŸ“– About memory growth](https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth)

### ğŸ“¦ Invalid box data error

```python
raise ValueError('Invalid box data. data must be a numpy array of '
ValueError: Invalid box data. data must be a numpy array of N*[y_min, x_min, y_max, x_max]
```

* ğŸ™„ For me it was a logical error, in `test_labels.csv` there were some invalid values like: **`file123.jpg,134,63,3,0,0,-1029,-615`**
* ğŸ· So, it was a labeling issue, fixing these lines solved the problem
* ğŸ‘€ [Related discussion](https://github.com/tensorflow/models/issues/3527) 

### ğŸ”„ Image with id  added issue

```python
raise ValueError('Image with id {} already added.'.format(image_id))
ValueError: Image with id 123.png already added.
```

* â˜ It is an issue in `.config` caused by giving value to `num_example` that is greater than total number of test image in test directory

```javascript
eval_config: {
  metrics_set: "coco_detection_metrics"
  use_moving_averages: false
  num_examples: 1265 // <--- this value was greater than total test images
}
```

## ğŸ§ References

* ğŸ“– [Training Custom Object Detector](https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/training.html#)
* ğŸ“– [TensorFlow Object Detection API](https://ai.yemreak.com/tensorflow-object-detection-api)
* ğŸ“– [Custom Object Detection using TensorFlow from Scratch](https://towardsdatascience.com/custom-object-detection-using-tensorflow-from-scratch-e61da2e10087)
* ğŸ“– [Supported object detection evaluation protocols](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/evaluation_protocols.md)

